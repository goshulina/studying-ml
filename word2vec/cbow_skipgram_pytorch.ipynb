{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO\n",
    " - CBOW negative sampling\n",
    " - TXTDatasets' __getitem__ - make convinient for bouth models\n",
    " - comments to dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([31079,  9553]),\n",
       " tensor([[26402, 32155, 17128, 12960],\n",
       "         [10328, 21830, 10530,  5701]]),\n",
       " tensor([[28396, 12114,  7512,  7679,  5026, 13825, 30690, 29736, 14684, 12741],\n",
       "         [ 9162, 24900, 18068, 14040, 12931, 13494, 17641, 20840, 10903, 15989]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TXTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        with open('/Users/georgychernousov/studying-ml/word2vec/reviews.txt','r') as f:\n",
    "            reviews = f.readlines()\n",
    "        self.words = ' '.join(reviews).split()[:1000000]\n",
    "        vocab = set(self.words)\n",
    "        vocab_size = len(vocab)\n",
    "        self.n_namples = vocab_size\n",
    "        self.word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
    "        self.ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "        self.neg_samples_amount = 10 # кратное контекстному окну\n",
    "        \n",
    "    ### cbow loader\n",
    "    # def __getitem__(self, index):\n",
    "    #     \"\"\"\n",
    "    #     return x, y, where x - context words, y target word\n",
    "    #     \"\"\"\n",
    "    #     left_context = self.words[index-2:index]\n",
    "    #     left_bias = 2 - len(left_context)\n",
    "    #     right_context = self.words[index+1:index+3+left_bias]\n",
    "    #     if len(right_context) < 2:\n",
    "    #         left_context.extend(self.words[index-4:index-2])\n",
    "    #     \n",
    "    #     context = torch.tensor([self.word_to_ix[i] for i in [*left_context, *right_context]], dtype=torch.long)\n",
    "    #     target = torch.tensor(self.word_to_ix[self.words[index]], dtype=torch.long)\n",
    "    #     return context, target\n",
    "    \n",
    "    ### skipgram loader\n",
    "    def __getitem__(self, index):\n",
    "        left_context = self.words[index-2:index]\n",
    "        left_bias = 2 - len(left_context)\n",
    "        right_context = self.words[index+1:index+3+left_bias]\n",
    "        if len(right_context) < 2:\n",
    "            left_context.extend(self.words[index-4:index-2])\n",
    "        \n",
    "        context = torch.tensor([self.word_to_ix[i] for i in [*left_context, *right_context]])\n",
    "        \n",
    "        center = self.word_to_ix[self.words[index]]\n",
    "        context_neg = np.random.choice(self.n_namples, self.neg_samples_amount, replace=False)\n",
    "        \n",
    "        return center, context, context_neg\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_namples\n",
    "    \n",
    "dataset = TXTDataset()\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=2, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "data_1 = dataiter.next()\n",
    "data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW model (without negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        self.init_emb(embedding_dim)\n",
    "        \n",
    "    def init_emb(self, embedding_dim):\n",
    "        \"\"\"\n",
    "        init the weight as original word2vec do.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        initrange = 0.5 / embedding_dim\n",
    "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds = torch.mean(embeds, dim=1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.in_emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "    def forward(self, center_word, context_words, neg_context):\n",
    "        '''\n",
    "        center_word: центральное слово, [batch_size,]\n",
    "        context_words: Слова вокруг окна контекста появляются вокруг [Batch_size * 2)]]\n",
    "        neg_context: нет слов вокруг центрального слова, от отрицательной выборки [batch_size, (window_size * 2 * k)]\n",
    "        return: loss\n",
    "        '''\n",
    "        center_word_emb = self.in_emb(center_word)  # [batch_szie,embed_size]\n",
    "        context_words_emb = self.out_emb(context_words)  # [batch,(2*C),embed_size]\n",
    "        neg_emb = self.out_emb(neg_context)  # [batch, (2*C * K),embed_size]\n",
    "            \n",
    "        log_pos = torch.bmm(context_words_emb,center_word_emb.unsqueeze(2)).squeeze()\n",
    "        log_neg = torch.bmm(neg_emb, -center_word_emb.unsqueeze(2)).squeeze()\n",
    "        \n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1)\n",
    "        \n",
    "        loss = log_pos + log_neg\n",
    "        return -loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, time 16.21 sec., total loss 332.0977602005005\n",
      "Epoch 1, time 15.20 sec., total loss 332.08157539367676\n",
      "Epoch 2, time 15.46 sec., total loss 332.0683536529541\n",
      "Epoch 3, time 16.01 sec., total loss 332.05159091949463\n",
      "Epoch 4, time 16.43 sec., total loss 332.0367784500122\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "EMDEDDING_DIM = 100\n",
    "model = CBOW(len(dataset), EMDEDDING_DIM)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "dataset = TXTDataset()\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for epoch in range(5):\n",
    "    start = dt.now()\n",
    "    trainingloss = 0\n",
    "    for context, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(context)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainingloss += loss.item()\n",
    "    print(f'Epoch {epoch}, time {(dt.now() - start).total_seconds():.2f} sec., total loss {trainingloss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, time 24.77 sec., total loss 1789.4531364440918\n",
      "Epoch 1, time 25.28 sec., total loss 1783.4533309936523\n",
      "Epoch 2, time 25.11 sec., total loss 1772.236515045166\n",
      "Epoch 3, time 23.39 sec., total loss 1772.7364120483398\n",
      "Epoch 4, time 23.45 sec., total loss 1765.78959274292\n",
      "Epoch 5, time 23.30 sec., total loss 1757.301284790039\n",
      "Epoch 6, time 24.06 sec., total loss 1760.1774139404297\n",
      "Epoch 7, time 24.75 sec., total loss 1749.7579040527344\n",
      "Epoch 8, time 24.64 sec., total loss 1739.440170288086\n",
      "Epoch 9, time 25.10 sec., total loss 1738.682041168213\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "EMDEDDING_DIM = 100\n",
    "model = SkipGram(len(dataset), EMDEDDING_DIM)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "dataset = TXTDataset()\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for epoch in range(10):\n",
    "    start = dt.now()\n",
    "    trainingloss = 0\n",
    "    for center, context, context_neg in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(center, context, context_neg).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainingloss += loss.item()\n",
    "    # if epoch % 1 == 0:\n",
    "    print(f'Epoch {epoch}, time {(dt.now() - start).total_seconds():.2f} sec., total loss {trainingloss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
